\documentclass[11pt]{report}

% ---------- Layout ----------
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}

% ---------- Graphics & Colors ----------
\usepackage{graphicx}
\usepackage{xcolor}

% Branding colors (pick one set of values and keep it consistent)
\definecolor{qcRed}{HTML}{E22237}   % Snapdragon Red
\definecolor{qcDark}{HTML}{101820}  % Dark neutral
\definecolor{qcGray}{HTML}{EDEDED}  % Light gray (optional)

% ---------- Utility Macros (branding) ----------
\newcommand{\qcrule}{\textcolor{qcRed}{\rule{16cm}{5pt}}}
\newcommand{\qctitle}[1]{\textcolor{qcRed}{\Huge\bfseries #1}}
\newcommand{\qcsectiontitle}[1]{\textcolor{qcDark}{\Large\bfseries #1}}

% ---------- Fonts & Headings ----------
\usepackage{titlesec}

% Chapter: inline style "1. Document Information"
\titleformat{\chapter}[hang]
  {\normalfont\huge\bfseries\color{qcRed}}
  {\thechapter.}
  {1em}
  {}
\titlespacing*{\chapter}{0pt}{-5pt}{15pt}

% Section formatting
\titleformat{\section}
  {\normalfont\Large\bfseries\color{qcDark}}
  {\thesection}
  {1em}
  {}
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{black}}
  {\thesubsection}
  {1em}
  {}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{black}}
  {\thesubsubsection}
  {1em}
  {}

% ---------- Tables / Lists ----------
\usepackage{array}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{enumitem}
\setlist[itemize]{topsep=2pt, partopsep=0pt, itemsep=2pt, parsep=0pt}
\setlist[enumerate]{topsep=2pt, partopsep=0pt, itemsep=2pt, parsep=0pt}

% ---------- TOC styling ----------
\usepackage{tocloft}
\renewcommand{\cfttoctitlefont}{\huge\bfseries\color{qcRed}} % TOC title red
\renewcommand{\cftchapfont}{\color{qcRed}}      % chapter entries red
\renewcommand{\cftchappagefont}{\color{qcRed}}  % chapter page numbers red
\renewcommand{\cftsecfont}{\color{black}}       % sections black
\renewcommand{\cftsecpagefont}{\color{black}}
\renewcommand{\cftsubsecfont}{\color{black}}    % subsections black
\renewcommand{\cftsubsecpagefont}{\color{black}}

% ---------- Hyperlinks (load LAST among most packages) ----------
\usepackage[hidelinks]{hyperref}
\usepackage{bookmark}
\hypersetup{
    pdfsubject={Driver Safety AI on Snapdragon},
    pdfkeywords={Snapdragon, Qualcomm, On-device AI, Driver Monitoring, Drowsiness Detection},
    colorlinks=false,
    linktoc=all,
    pdfstartview=FitH,
    bookmarksopen=true,
    bookmarksnumbered=true
}

%Cover Page
% (pdfpages already loaded above indirectly; ensure single load and avoid
% duplicate hyperref loading to keep PDF bookmarks stable)
\usepackage{pdfpages}
\newcommand{\req}[1]{\hyperlink{#1}{\texttt{#1}}} % inline link macro


% ---------- Your Vars ----------
\def\myversion{1.0}

\setcounter{secnumdepth}{3}  % Number down to \subsubsection
\setcounter{tocdepth}{3}     % Include in the Table of Contents (optional)
\usepackage{float}

\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2265}{$\ge$}


\begin{document}

% --- Capstone cover page (from PDF) ---
\includepdf[
  pages=1,                       % just the first page of the PDF
  pagecommand={\thispagestyle{empty}}, % ensure no headers/footers/bookmarks on cover
  fitpaper=true,                 % scale to full page
]{assets/CSU-SM-CSE-39-2026-SE-001-Team-009.pdf}

\clearpage

% reset numbering to start after the cover
\pagenumbering{arabic}
\setcounter{page}{1}



% ========================= TOC =========================
\tableofcontents
{\color{qcRed}\rule{16cm}{2pt}}

\clearpage

% ========================= Abstract =========================
\chapter{Abstract}
\textit{ALVION} is an Android app that spots signs of driver drowsiness and distraction using a phone’s front-facing camera and on-device machine learning. The idea is simple: many new cars include driver monitoring, but millions of older vehicles do not. Our goal is to close that gap with an accessible aftermarket option that runs entirely on Snapdragon-based smartphones, without sending any video to the cloud. Running locally keeps latency low for timely alerts, protects privacy by keeping data on the device, and showcases what modern mobile hardware can do in real-time.

The system analyzes live video to track visual cues linked to fatigue and inattention, applies configurable thresholds, and triggers clear alerts through sound, screen prompts, and vibration when attention appears to drift. The design focuses on what we can build and test in a years time: reliable on-device inference, straightforward configuration, and a responsive alert pipeline. We do not attempt vehicle integration, cloud processing, large-scale deployment, or regulatory certification.

This document lays out the project’s objectives, assumptions, and boundaries so developers, mentors, and faculty can stay aligned as we move from concept to implementation. It explains the system architecture, component responsibilities, and the choices behind them, aiming for a design that is practical to build now and easy for a future team to extend. In short, ALVION demonstrates that a phone you already carry can provide meaningful safety signals when designed with careful attention to latency, privacy, and usability.

% ========================= DOC INFO =========================
\chapter{Document Information}

\section{Report Revision History}
\begin{table}[h]
\centering
\begin{tabular}{|p{2cm}|p{3cm}|p{7.8cm}|p{3cm}|}
\hline
\textbf{Version} & \textbf{Date} & \textbf{Description} & \textbf{Author} \\
\hline
1.0 & \multicolumn{1}{c|}{Oct 13, 2024} & Initial draft submitted. Report outline aligned to capstone guideline; 200–300 word abstract finalized; Introduction completed (Purpose, Background, Scope, Intended Audience, Project Description). Scope and constraints defined for on-device Snapdragon prototype. Document metasetup added (revision history and approval tables). & Team 9\\
\hline
1.5 & \multicolumn{1}{c|}{Oct 31, 2024} &  & Team 9\\
\hline
2.0 & \multicolumn{1}{c|}{Dec 1, 2024} &  & Team 9\\
\hline
\end{tabular}
\end{table}

\clearpage

% ========================= 3. PROBLEM & CONTEXT =========================
\chapter{Problem Statement}


\section{Business Background}
Driver monitoring is becoming a regulatory and industry standard for road safety. The European Union now mandates such systems in new vehicles beginning in 2024, and other markets are expected to follow. Automakers and regulators view these systems as essential for reducing human-error-related crashes, but adoption in existing vehicles remains limited because of cost, integration complexity, and consumer concerns over data privacy.
Retrofit products, while available, often require expensive hardware installations or continuous cloud connectivity for processing. These barriers restrict adoption, especially in regions where consumers rely on older cars or are reluctant to share biometric data online. A smartphone-based solution reduces those barriers: most users already own compatible Android devices with capable cameras and neural-processing hardware.
ALVION’s approach uses these resources to demonstrate how Qualcomm’s mobile AI platforms can perform effective, real-time driver-monitoring inference entirely on-device. This provides a testbed for cost-effective safety innovations and shows potential business alignment with regulatory trends. The project also establishes a baseline for future Qualcomm-enabled automotive safety research and gives developers a model for edge-based AI deployment that respects user privacy and reduces infrastructure costs.

\clearpage

\section{Needs}
Improving driver safety requires early detection of distraction and drowsiness in a manner that is both reliable and user-friendly. ALVION responds to several intertwined needs:

\begin{itemize}
  \item \textbf{Safety:} The system should protect both the driver and other road users by recognizing behavioral cues such as prolonged eyelid closure, yawning, or head tilting that precede dangerous inattention.
  \item \textbf{Privacy:} Consumers are wary of surveillance systems that transmit facial data to the cloud. An entirely on-device approach ensures that personal information remains secure while still providing effective monitoring.
  \item \textbf{Accessibility and Cost:} Full-scale automotive integrations are expensive. By relying on common smartphone hardware, ALVION provides an affordable entry point for drivers worldwide.
  \item \textbf{Responsiveness:} Alerts must reach the driver when they matter most. Real-time analysis ensures immediate feedback that can prevent accidents instead of only documenting them.
  \item \textbf{Reusability and Scalability:} The project serves as an open reference implementation that can be adapted, expanded, and optimized by future teams, supporting continuous improvement in mobile AI safety systems.
\end{itemize}
\clearpage

\section{Objectives}
ALVION’s objectives focus on producing a working, reproducible prototype that demonstrates the technical and practical viability of phone-based driver monitoring:

\begin{itemize}
  \item \textbf{Develop a proof-of-concept Android application} capable of detecting drowsiness and distraction using the front camera and a lightweight on-device model optimized for Snapdragon NPUs.
  \item \textbf{Implement configurable alert mechanisms} that combine audio, visual, and haptic feedback, allowing users to tune thresholds and notification styles for comfort and responsiveness.
  \item \textbf{Achieve efficient real-time performance} without dependence on cloud computing, ensuring low latency and minimal power usage consistent with mobile operation.
  \item \textbf{Document all design decisions, architecture, and testing methodology} in a reproducible form so that future development teams can extend or refine the system.
  \item \textbf{Conduct iterative testing and evaluation} under varying lighting, motion, and device conditions to measure accuracy, usability, and overall system robustness.
\end{itemize}

Through these objectives, ALVION demonstrates a concrete pathway from concept to functional prototype, aligning technical feasibility with ethical design and industry safety goals.
\clearpage


% ========================= 4. USER REQUIREMENTS =========================
\chapter{Requirements}
\section{User Requirement}
\subsection{Glossary of Relevant Domain Terminology}

\begin{description}

  \item[ALVION] The Android-based driver drowsiness and distraction detection mobile application developed for this capstone project. Runs entirely on-device to ensure privacy and real-time performance.

  \item[Driver Monitoring System (DMS)] A safety technology that uses sensors or cameras to assess driver attention, alertness, and potential fatigue while operating a vehicle.

  \item[PERCLOS (Percentage of Eyelid Closure)] A standard metric used in drowsiness detection. It represents the proportion of time a driver’s eyes are at least 80\% closed over a given time window.

  \item[Head Yaw] The horizontal rotation of a driver’s head relative to the camera. Excessive yaw over time indicates potential distraction or inattention.

  \item[Android Studio] The official integrated development environment (IDE) for Android applications. Used to develop and build the ALVION mobile app using Kotlin.

  \item[Kotlin] A modern programming language fully supported by Android, known for concise syntax and null safety. Used as the primary implementation language for ALVION.

  \item[Jetpack Compose] Android’s declarative UI framework written in Kotlin. It simplifies the creation of reactive, adaptive user interfaces without the need for XML layout files.

  \item[CameraX] A Jetpack library that provides easy access to camera functions such as preview, capture, and frame analysis. Used in ALVION to stream frames for real-time inference.

  \item[TensorFlow Lite (TFLite)] A lightweight, cross-platform runtime for executing trained machine learning models on mobile and embedded devices. Handles on-device inference in ALVION.

  \item[NNAPI (Neural Networks API)] Android’s standard interface that allows apps to access on-device accelerators such as GPUs, DSPs, or NPUs for running ML workloads efficiently.

  \item[Qualcomm Neural Network (QNN) Runtime] Qualcomm’s proprietary runtime for accelerating AI models on Snapdragon’s Hexagon DSP/HTP cores. Used optionally for performance optimization.

  \item[ML Kit] Google’s on-device machine learning SDK that provides ready-to-use APIs for vision tasks such as face detection, eye openness estimation, and head pose detection.

  \item[MediaPipe] An open-source framework for building cross-platform ML pipelines, including facial landmark detection. Used as an alternative to ML Kit for greater control over signals.

  \item[OpenCV] Open-source computer vision library providing image-processing tools such as resizing, cropping, and overlay drawing. Used optionally for debugging or preprocessing.

  \item[PyTorch] A machine learning framework used for training models off-device. In ALVION, it can be used to train lightweight classifiers before conversion to TFLite.

  \item[ONNX (Open Neural Network Exchange)] A standardized model format that enables conversion and interoperability between ML frameworks like PyTorch and TensorFlow.

  \item[Quantization] A model optimization technique that reduces numerical precision (e.g., from 32-bit floats to 8-bit integers) to improve speed and reduce memory use during inference.

  \item[Debounce Logic] A software mechanism that prevents repeated triggers of an alert within a short time window, reducing false positives in drowsiness detection.

  \item[Sliding Window Aggregation] A temporal data analysis technique that aggregates frame-level results (e.g., eye openness) over a fixed time window to detect sustained patterns such as drowsiness.

  \item[BYOM (Bring Your Own Model)] A Qualcomm workflow allowing developers to integrate custom-trained ML models into Snapdragon devices using the QNN SDK.

  \item[MVP (Minimum Viable Product)] The simplest version of the app that demonstrates key functionality (on-device detection, alerts, and adjustable thresholds) without full feature expansion.

  \item[Latency] The time delay between camera frame capture and alert output. Low latency is critical to ensure timely driver warnings.

  \item[FPS (Frames Per Second)] A measure of how many frames are processed per second during video analysis. Maintaining $\geq$15 FPS ensures smooth real-time inference.

  \item[On-device Inference] The process of running AI models locally on the phone rather than sending data to cloud servers. Essential for preserving privacy and minimizing delay.

  \item[Privacy by Design] A principle ensuring that user data (especially facial video) remains on the device and is not transmitted externally without explicit consent.

  \item[Thermal Throttling] Automatic performance reduction that occurs when a device’s temperature rises beyond safe limits. ALVION monitors this to adjust inference rate dynamically.

  \item[Drowsiness Alert Cooldown] A configurable delay after an alert during which no new alerts are triggered, preventing repetitive warnings and user annoyance.

  \item[Subject-wise Split] A data handling practice in ML ensuring that images or videos of the same person appear only in either training or testing sets, avoiding data leakage.

  \item[Dataset Licensing (Research Use Only)] Legal restrictions defining how public datasets (e.g., CEW, MRL, NTHU-DDD) can be used. ALVION uses only datasets with academic/research permissions.

\end{description}




\subsection{User Groups}
% Primary driver (legacy vehicle owner), tester, demo user.
This section describes the primary user groups who will interact with the Distracted/Drowsy Driving Detection System. Each group represents a distinct role with unique goals and responsibilities related to the system's functionality and operation.
\begin{itemize}    \item {Drivers:} Main end users; rely on alerts
    \item {Fleet Managers / Supervisors:} oversee multiple drivers, review aggregated logs
    \item {System Administrators:} Maintain the deployment, handle privacy configs and updates
    \item {Developers / Maintenance Engineers (Internal):} internal users who maintain and enhance the app
    
\end{itemize}

\subsection{Project Context}
\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{assets/context_diagram_v1.png}
\caption{System context Diagram for the AlVION driver drowsiness and distraction detection mobile application}
\label{fig:context}
\end{figure}

\noindent
The context diagram illustrates how the ALVION mobile application interacts with its surrounding environment. 
The driver provides a live video feed through the smartphone camera and receives real-time alerts through audio, vibration, or visual cues. 
The application communicates with the Android OS using standard APIs for camera access, vibration control, and storage management. 
It performs on-device inference via the TensorFlow Lite runtime accelerated through Qualcomm’s QNN or Android’s NNAPI interface. 
Optional connections—such as aggregated summaries for fleet managers, managed configurations for system administrators, or diagnostic feedback for developers—represent potential future extensions.
  
\subsection{Functional Requirements}

\subsubsection{Project Scope}
\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{assets/project_scope_v1.png}
\caption{Scope diagram to illustrate major user interactions with ALVION}
\label{fig:scope}
\end{figure}

\subsubsection{User Scenarios}
The following scenarios describe how different actors interact with the ALVION mobile application.  
Each scenario corresponds to a use case identified in the Project Scope diagram and summarized in Appendix~U.

\begin{itemize}

  \item \textbf{Start Monitoring Session:}  
  The driver launches the ALVION application, grants camera permissions, and begins a monitoring session. The app activates the front-facing camera through CameraX and initializes on-device ML inference to begin analyzing the driver’s eye openness and head orientation in real time.

  \item \textbf{Receive Drowsiness Alert:}  
  During monitoring, ALVION continuously evaluates the Percentage of Eyelid Closure (PERCLOS). If the eyes remain closed beyond the configured duration threshold, the system issues an audible and visual alert to prompt the driver to regain focus.

  \item \textbf{Receive Distraction Alert:}  
  The application monitors head yaw angle using the face-detection model. If the driver’s head remains turned away from the forward direction for longer than the threshold period, a distraction alert is triggered.

  \item \textbf{Acknowledge or Snooze Alert (Extend):}  
  After receiving a drowsiness or distraction alert, the driver can choose to acknowledge it or temporarily snooze further alerts for a short cooldown period to prevent repetitive warnings.

  \item \textbf{Adjust Sensitivity and Settings:}  
  The driver opens the Settings screen to adjust thresholds for eye-closure time, yaw angle, or alert cooldown. These preferences are stored locally and persist between sessions.

  \item \textbf{Calibrate / Align Camera (Include):}  
  On first use or when lighting changes, the driver may be prompted to align the camera for optimal face tracking. The app displays live feedback until proper alignment is achieved.

  \item \textbf{View Alert Summary (Optional):}  
  After a session ends, the driver can view a local summary showing the number and type of alerts triggered. This information helps evaluate driving habits or fatigue patterns.

  \item \textbf{Export Logs (Optional):}  
  Users can export anonymized event logs to share with researchers or safety supervisors. This function remains offline and requires explicit user consent.

  \item \textbf{Manage App Deployment (System Administrator):}  
  The system administrator deploys updates, manages app permissions, and enforces organizational settings through Android device management tools.

  \item \textbf{View Aggregated Reports (Fleet Manager):}  
  In a potential future version, fleet managers could access a dashboard summarizing aggregated alert data across multiple drivers to assess safety trends and rest policies.

\end{itemize}

\subsubsection{User Functional Requirements}
This section provides short descriptions of the user functional requirements identified for the ALVION mobile application.  
Each requirement corresponds to the detailed tables presented in Appendix~R and defines a specific capability the system shall provide to meet user and stakeholder goals.

\begin{enumerate}

  \item \textbf{UF-A: Account Management} —  
  The system shall allow a driver to create an account and securely log in to view personal driving history or session data.  

  \item \textbf{UF-B: Drowsiness Detection and Alerting} —  
  The system shall continuously analyze eye openness using the PERCLOS metric and issue a warning when sustained eyelid closure indicates drowsiness.

  \item \textbf{UF-C: Distraction Detection and Alerting} —  
  The system shall detect when the driver’s head or gaze deviates from the forward direction for longer than a defined interval and trigger an attention-restoring alert.

  \item \textbf{UF-D: Configurable Sensitivity and Alert Settings} —  
  The system shall provide a settings interface that allows the driver to adjust detection sensitivity, alert volume, and cooldown timing.

  \item \textbf{UF-E: Start and Stop Monitoring Session} —  
  The driver shall be able to manually start and end a monitoring session, activating or releasing the camera and inference modules as needed.

  \item \textbf{UF-F: Real-Time Alert Delivery} —  
  The system shall deliver real-time visual, audio, or vibration alerts when drowsiness or distraction is detected so that the driver can immediately respond.

  \item \textbf{UF-G: Acknowledge or Snooze Alerts} —  
  The system shall provide options for the driver to acknowledge or temporarily snooze alerts, reducing repeated notifications during recovery periods.

  \item \textbf{UF-H: Camera Calibration and Alignment} —  
  The system shall provide a calibration routine to ensure the driver’s face is properly framed and visible before or during monitoring to improve detection accuracy.

  \item \textbf{UF-I: Session Summary and Export (Optional)} —  
  The system shall display a local summary of each session’s alerts and optionally export anonymized logs for driver review or research use.

  \item \textbf{UF-J: Privacy and Local Data Control} —  
  The system shall store all captured data locally on the device and prevent external transmission unless the user explicitly consents.

  \item \textbf{UF-K: Administrative Deployment and Configuration} —  
  Authorized system administrators shall be able to deploy application updates and manage configuration policies through Android enterprise tools.

  \item \textbf{UF-L: Aggregated Fleet Reporting} —  
  In future releases, the system may provide a fleet-manager dashboard that aggregates driver alert statistics to support organizational safety analysis.

\end{enumerate}

\subsection{Non-Functional Requirements}
This section summarizes the non-functional requirements defined for the ALVION driver drowsiness and distraction detection application.  
Each group below corresponds to the detailed requirement tables in Appendix~R and captures essential quality, organizational, and external constraints that guide the project.

\subsubsection{Product: Usability Requirements}

\begin{itemize}
  \item \textbf{UP-01: Simple User Interface} – The application shall provide an intuitive and minimal interface that allows users to begin monitoring with minimal interaction.
  \item \textbf{UP-02: Hands-Free Operation} – During active monitoring, no user input shall be required; all alerts and responses shall occur automatically through audio or vibration feedback.
  \item \textbf{UP-03: Clear Feedback and Alerts} – Visual, auditory, and haptic feedback shall be easily distinguishable and clearly convey the driver’s attention status.
  \item \textbf{UP-04: Accessibility Support} – Interface text, color, and contrast shall follow accessibility guidelines to remain legible under varied lighting conditions.
\end{itemize}

\subsubsection{Product: Performance Requirements}

\begin{itemize}
  \item \textbf{UP-05: Frame Rate and Latency} – The system shall process at least 15~frames per second and maintain an alert latency below 200~ms.
  \item \textbf{UP-06: On-Device Inference Efficiency} – All model inference shall execute locally on Snapdragon CPU, GPU, or NPU with average CPU utilization below 40\%.
  \item \textbf{UP-07: Thermal and Battery Management} – The system shall reduce inference frequency if device temperature or power consumption exceeds defined limits.
\end{itemize}

\subsubsection{Product: Availability, Reliability, and Security Requirements}

\begin{itemize}
  \item \textbf{UP-08: Operational Availability} – The application shall run continuously for up to two hours without crash or data loss.
  \item \textbf{UP-09: Fail-Safe Behavior} – In the event of hardware or model failure, the system shall suspend monitoring and notify the user instead of issuing false alerts.
  \item \textbf{UP-10: Data Privacy and Security} – All logs and image data shall remain on-device in app-scoped storage; no external transmission shall occur without explicit consent.
  \item \textbf{UP-11: Permission Control} – The app shall request only essential Android permissions and respect user revocation at runtime.
\end{itemize}

\subsubsection{Organizational: Development Requirements}

\begin{itemize}
  \item \textbf{UO-01: Development Environment} – The project shall be developed in Android Studio using Kotlin and tested on Snapdragon hardware to ensure compatibility.
  \item \textbf{UO-02: Version Control} – All source code shall be managed through a Git repository with standardized branching and commit practices.
  \item \textbf{UO-03: Coding Conventions} – Code shall follow Kotlin style guidelines and include inline documentation for maintainability.
  \item \textbf{UO-04: Testing Coverage} – Each functional requirement shall be verified through at least one unit or integration test prior to release.
\end{itemize}

\subsubsection{Organizational: Operational Requirements}

\begin{itemize}
  \item \textbf{UO-05: Deployment Compatibility} – The system shall be distributed as an Android APK targeting API Level~33 or higher and installable without root access.
  \item \textbf{UO-06: User Support Documentation} – The project shall provide a concise user guide or in-app help section describing calibration, settings, and safety disclaimers.
\end{itemize}

\subsubsection{Organizational: Environmental Requirements}

\begin{itemize}
  \item \textbf{UO-07: Lighting Adaptability} – The app shall function accurately under daylight, artificial light, and low-light cabin conditions.
  \item \textbf{UO-08: Device Orientation} – The application shall operate correctly when the device is mounted in either landscape or portrait orientation.
  \item \textbf{UO-09: Offline Functionality} – All monitoring and alert functions shall operate fully without network connectivity.
\end{itemize}

\subsubsection{External: Legislative Requirements on Safety and Security}

\begin{itemize}
  \item \textbf{UE-01: Driver Safety Compliance} – The application shall include a startup safety disclaimer and disable manual interaction while the vehicle is in motion.
  \item \textbf{UE-02: Data Protection Compliance} – The system shall comply with applicable privacy laws such as CCPA and GDPR regarding consent and data retention.
\end{itemize}

\subsubsection{External: Cultural and Social Requirements}

\begin{itemize}
  \item \textbf{UE-03: Inclusive Model Performance} – The detection model shall be evaluated for consistent accuracy across diverse skin tones, facial structures, and eyewear.
  \item \textbf{UE-04: Language Localization} – The user interface shall default to English and be structured for easy translation into additional languages.
\end{itemize}

\subsubsection{External: Interoperability Requirements}

\begin{itemize}
  \item \textbf{UE-05: Android API Compatibility} – The application shall utilize stable Android Jetpack APIs such as CameraX, AudioManager, and Storage to ensure broad device support.
  \item \textbf{UE-06: Hardware Acceleration Interfaces} – The application shall remain interoperable with both Qualcomm QNN and Android NNAPI runtimes for accelerated on-device inference.
\end{itemize}

\clearpage

% ========================= 4. SYSTEM REQUIREMENTS =========================
\section{\textcolor{qcRed}{System Requirements}}

% ---------------- 4.2.1 Functional Requirements ----------------
\subsection{Functional Requirements}

% 4.2.1.1 System Functional Requirements
\subsubsection{System Functional Requirements}
The ALVION system shall implement the following core functional features to enable real-time driver monitoring and alerting:

\begin{itemize}
  \item \textbf{SF-01: Real-Time Drowsiness Detection} —  
  The system shall continuously analyze the driver’s eye openness using the Percentage of Eyelid Closure (PERCLOS) metric to determine signs of drowsiness. When the eyes remain closed beyond a configurable threshold, a drowsiness alert shall be triggered.

  \item \textbf{SF-02: Distraction Detection} —  
  The system shall track the driver’s head yaw (horizontal rotation) and issue a distraction alert when the driver looks away from the forward direction for more than a defined dwell time.

  \item \textbf{SF-03: Alert Mechanisms} —  
  The system shall deliver real-time alerts using audio tones, vibration feedback, and visual overlay cues to ensure the driver’s attention is regained promptly.

  \item \textbf{SF-04: Configurable Sensitivity and Thresholds} —  
  The system shall allow users to adjust detection sensitivity, eyelid-closure duration, yaw angle threshold, and alert cooldown periods via the Settings interface.

  \item \textbf{SF-05: Camera Calibration and Alignment} —  
  On initial launch or when lighting conditions change, the system shall prompt the user to align the front-facing camera for optimal face tracking and detection accuracy.

  \item \textbf{SF-06: Session Control} —  
  Users shall be able to start and stop monitoring sessions manually, enabling or releasing the camera and inference modules as needed.

  \item \textbf{SF-07: Snooze and Acknowledge Alerts} —  
  Users shall be able to acknowledge or temporarily snooze alerts to prevent repeated notifications during rest or recovery.

  \item \textbf{SF-08: Session Summary and Reporting} —  
  After each session, the app shall display a local summary including the number and type of alerts triggered, session duration, and driver responsiveness metrics.

  \item \textbf{SF-09: Privacy and Data Protection} —  
  All facial analysis and inference shall occur locally on the device. No video or image data shall be transmitted externally without explicit user consent.

  \item \textbf{SF-10: Administrative Configuration} —  
  System administrators shall be able to configure and deploy the app across managed Android devices using enterprise deployment tools and preset configurations.
\end{itemize}

% ---------------- 4.2.1.2 Data Requirements ----------------
\subsubsection{Data Requirements}
The ALVION application processes and manages several categories of data required for on-device drowsiness and distraction detection. These data requirements describe what information is captured, derived, stored, and protected within the system.

\paragraph{Data Inputs}
\begin{itemize}
  \item \textbf{Camera Frames:} RGB frames captured from the front-facing camera (resolution up to 1920×1080, $\ge$15 FPS) using the CameraX API.
  \item \textbf{Sensor Metadata:} Device orientation and timestamp values from Android SensorManager for head-pose compensation.
\end{itemize}

\paragraph{Data Processing and Outputs}
\begin{itemize}
  \item \textbf{Processed Metrics:} Per-frame eye-openness ratio (PERCLOS), head yaw/pitch angles, and attention state labels (alert / drowsy / distracted).
  \item \textbf{Alerts:} Boolean and string messages delivered to the UI and audio subsystem indicating driver status or safety warnings.
\end{itemize}

\paragraph{Data Storage Requirements}
\begin{itemize}
  \item \textbf{Local Storage:} Session summaries and calibration data stored in JSON or SQLite format under app-scoped storage. No cloud database is used.
  \item \textbf{Retention:} Logs persist until manually deleted by the user. Temporary inference buffers are discarded after each session.
\end{itemize}

\paragraph{Data Relationships}
Session data link driver ID, timestamps, and alert metrics to a single session record:
\[
\textit{Session}(session\_id, timestamp, perclos\_avg, alert\_count, duration)
\]
Processed data are read-only to other modules; only summary statistics are exposed to the UI.

\paragraph{Data Quality and Integrity}
\begin{itemize}
  \item All timestamps follow ISO 8601 format.
  \item Invalid or missing sensor frames are ignored, not extrapolated.
  \item Feature values are clamped to physical limits (0 $\le$ PERCLOS $\le$ 1).
\end{itemize}

\paragraph{Data Privacy and Security}
\begin{itemize}
  \item All data remain on-device in encrypted storage (UP-10, UE-02).
  \item No identifiable images or raw video are transmitted externally.
  \item Inference executes locally using Qualcomm QNN / Android NNAPI.
\end{itemize}

\textit{These data specifications define the system’s information model and handling rules. They are not user functional requirements but underpin compliance with non-functional constraints on privacy, performance, and reliability.}

% ---------- 4.2.2 Non-functional Requirements ----------
\subsection{Non-functional Requirements}


% 4.2.3.1 Product: Usability Requirements
\subsubsection{Product: Usability Requirements}
\begin{itemize}
  \item \textbf{SP-01-01} — The application shall provide a simple, intuitive interface that allows users to start monitoring with minimal setup.
  \item \textbf{SP-02-01} — The system shall require no manual input during active monitoring, ensuring fully hands-free operation.
  \item \textbf{SP-03-01} — Alerts shall be clearly distinguishable through visual, auditory, or haptic feedback, synchronized within $\pm$50\,ms.
  \item \textbf{SP-04-01} — The interface shall maintain readability and accessibility under different lighting conditions and conform to WCAG~2.1~AA.
\end{itemize}

% 4.2.3.2 Product: Performance Requirements
\subsubsection{Product: Performance Requirements}
\begin{itemize}
  \item \textbf{SP-05-01} — The system shall process at least 15 frames per second and issue alerts within 200\,ms of detection.
  \item \textbf{SP-05-02} — Average CPU utilization during active monitoring shall remain below 40\%, and the application shall adapt processing rates to manage battery and thermal constraints.
\end{itemize}

% 4.2.3.3 Product: Availability/Reliability/Security
\subsubsection{Product: Availability/Reliability/Security}
\begin{itemize}
  \item \textbf{SP-08-01} — The application shall remain operational for up to two hours without crash or data loss.
  \item \textbf{SP-09-01} — If the camera or ML inference fails, the system shall pause monitoring and notify the user.
  \item \textbf{SP-10-01} — All logs and configuration files shall be stored in secure app-scoped encrypted storage and deletable on request.
  \item \textbf{SP-11-01} — The application shall respect Android runtime permission revocations and immediately disable affected capabilities.
\end{itemize}

% 4.2.3.4 Organizational: Development Requirements
\subsubsection{Organizational: Development Requirements}
\begin{itemize}
  \item \textbf{SO-01-01} — The project shall be implemented using Android Studio (Kotlin) with Git version control.
  \item \textbf{SO-04-01} — Continuous-integration builds shall enforce unit-test coverage $\ge$\,80\%.
\end{itemize}

% 4.2.3.5 Organizational: Operational Requirements
\subsubsection{Organizational: Operational Requirements}
\begin{itemize}
  \item \textbf{SO-05-01} — The system shall be distributed as an APK targeting Android API Level~33 or higher.
  \item \textbf{(from UO-06)} — The application shall include an in-app guide explaining setup, calibration, and safety disclaimers.
\end{itemize}

% 4.2.3.6 Organizational: Environmental Requirements
\subsubsection{Organizational: Environmental Requirements}
\begin{itemize}
  \item \textbf{SO-07-01/02} — The system shall function reliably under daylight, artificial, and low-light conditions.
  \item \textbf{SO-08-01} — The application shall operate correctly in both portrait and landscape orientations.
  \item \textbf{SO-09-01} — All detection and alerting functionality shall remain available without network access.
\end{itemize}

% 4.2.3.7 External: Legislative Requirements on Safety/Security
\subsubsection{External: Legislative Requirements on Safety/Security}
\begin{itemize}
  \item \textbf{SE-01-01} — The application shall display a startup safety disclaimer and disable interactive prompts while the vehicle is moving.
  \item \textbf{(from UE-02)} — Stored data shall comply with GDPR/CCPA consent, deletion, and retention rules.
\end{itemize}

% 4.2.3.8 External: Cultural and Social Requirements
\subsubsection{External: Cultural and Social Requirements}
\begin{itemize}
  \item \textbf{SE-03-01} — The detection model shall be validated for consistent performance across diverse skin tones, facial features, and eyewear.
  \item \textbf{SE-04-01} — The user interface shall default to English and be structured for easy localization.
\end{itemize}

% 4.2.3.9 External: Interoperability Requirements
\subsubsection{External: Interoperability Requirements}
\begin{itemize}
  \item \textbf{SE-05-01} — The system shall use stable Android Jetpack APIs such as CameraX, AudioManager, and Storage for broad compatibility.
  \item \textbf{SE-06-01} — The ML inference pipeline shall interoperate with both Qualcomm QNN and Android NNAPI runtimes for accelerated on-device inference.
\end{itemize}



% ========================= 5. EXPLORATORY STUDIES =========================
\chapter{Exploratory Studies}\label{ch:exploratory-studies}

% ---------- 5.1 Relevant Development Frameworks ----------
\section{Relevant Development Frameworks}\label{sec:frameworks}
We are building a phone-first prototype that runs entirely on the device. Each tool below has one clear job. We avoid extra moving parts so we can ship fast and keep the app stable on a Galaxy phone.

\subsection{Android Studio (Kotlin)}
\textbf{What it is.} Android Studio is the IDE; \textbf{Kotlin} is the language for our app.  
\textbf{Why we use it.} It’s the standard way to build high-quality Android apps and gives us first-class access to the camera, sensors, and hardware acceleration.

\subsection{Front end: Jetpack Compose (native UI)}
\textbf{What it is.} Compose is Kotlin’s modern UI toolkit (native, declarative), similar in feel to React, but it compiles to Android views directly.  
\textbf{Why we use it.} We can design clean, responsive screens (camera preview overlay, status badges, settings) without XML. Compose + Material3 gives us polished components out of the box.  
\textbf{UI libs we rely on (lightweight):} \emph{material3} (design system), \emph{accompanist-permissions} (runtime camera permission), \emph{coil} (optional image loading for debug).

\subsection{Camera: CameraX (preview + frames for ML)}
\textbf{What it is.} A high-level camera library on top of Camera2.  
\textbf{Why we use it.} It gives us (1) a live preview for the UI and (2) an “analysis” stream of frames for ML—without wrestling with low-level sessions. It also handles rotation and lifecycle, and lets us “keep only latest” frames so the app stays smooth.

\subsection{On-device inference: TensorFlow Lite (TFLite)}
\textbf{What it is.} The small runtime that \emph{runs} a model file on the phone.  
\textbf{Where it runs.} On the Galaxy device, inside our Kotlin app.  
\textbf{Why we use it.} TFLite is portable and works with Android’s \textbf{NNAPI} so the phone can use hardware accelerators when available and CPU when not.  
\textbf{Optional optimization.} On Snapdragon devices that expose the Hexagon/HTP path, we can enable \textbf{Qualcomm QNN} as a feature flag for extra speed/power savings. If QNN isn’t available, the app still runs via TFLite+NNAPI.

\subsection{Vision helpers: ML Kit / MediaPipe / OpenCV}
\textbf{ML Kit Face Detection.} Fast path: gives eye-open probabilities and head yaw directly—no training needed.  
\textbf{MediaPipe Face Landmarker.} Alternative that returns facial landmarks so we can compute eye openness (EAR) and yaw ourselves.  
\textbf{OpenCV (optional).} A toolbox for basic image tasks (resize, crop, color convert, draw debug overlays). We add it only if we need it, behind a small Kotlin interface, so the rest of the app stays the same.

\subsection{Training (off-device, only if needed): PyTorch \texorpdfstring{$\rightarrow$}{->} ONNX \texorpdfstring{$\rightarrow$}{->} TFLite}
\textbf{What it is.} \textbf{PyTorch} is used on a laptop/Colab to \emph{train} a tiny eye-state model if field tests show we need one.  
\textbf{Where it runs.} Off-device. The phone does \emph{not} train.  
\textbf{Why we use it.} PyTorch is quick to iterate. We export PyTorch $\to$ ONNX $\to$ \textbf{TFLite (INT8)} so the Android app can run the model with TFLite+NNAPI/QNN. If post-training quantization reduces accuracy too much, we try light QAT; otherwise we keep PTQ for simplicity.

\subsection{Project glue, performance, and stability}
\textbf{Gradle} builds the app. \textbf{Kotlin coroutines/Flow} keep ML off the UI thread. The analyzer uses “\emph{keep only latest}” so we never build a big queue—this bounds latency and avoids stutter. We monitor timing and thermals with \textbf{Android Profiler} and \textbf{Perfetto}; if the phone warms up, we downshift internal inference rate (e.g., 30 $\rightarrow$ 24 FPS) while keeping the UI responsive.

\subsection*{Tool roles at a glance}
\begin{center}
\begin{tabular}{p{3.2cm} p{5.3cm} p{7.2cm}}
\toprule
\textbf{Tool} & \textbf{Role} & \textbf{Why it’s in the stack} \\
\midrule
Android Studio + Kotlin & App development & Official Android toolchain; fastest path to a stable, native app\\
\\
Jetpack Compose (Material3) & Front-end UI & Native, declarative UI; polished components; easy to iterate\\
\\
CameraX & Camera preview + frame analysis & Fewer device quirks; easy lifecycle; analysis frames for ML, backpressure control\\
\\
TFLite (+NNAPI) & On-device inference & Portable runtime; uses accelerators when available; sane CPU fallback\\
\\
QNN (optional) & Snapdragon acceleration & Feature flag for extra speed/power on supported devices\\
\\
ML Kit / MediaPipe & Ready facial signals & Eye-open probability / landmarks without training; jump-start MVP\\
\\
OpenCV (optional) & Image utilities & Only if we need extra preprocessing or debug overlays\\
\\
PyTorch (off-device) & Training (if needed) & Fast to prototype a tiny eye-state model; exports to TFLite for the app \\
\bottomrule
\end{tabular}
\end{center}

% ---------- 5.2 Relevant Solution Techniques & Dataset Strategy ----------
\section{Relevant Solution Techniques \& Dataset Strategy}\label{sec:solutions}
Our objective is a reliable, real-time demo on a Snapdragon phone. We will \emph{not} train from scratch. Instead, we use a tiered approach that prioritizes fast geometric signals on-device and selectively leverages an existing MobileNetV2 model trained on a large Kaggle dataset.

\subsection{Tiered Runtime (device-first)}
\paragraph{Tier 1 (default, no training).}
Face landmarks on device (e.g., ML Kit or MediaPipe) to estimate:
\begin{itemize}
  \item \textbf{Eye openness:} framewise openness per eye; PERCLOS over a sliding window for drowsiness.
  \item \textbf{Head yaw (left/right):} hysteresis + timeout to flag distraction rather than micro head jitters.
  \item \textbf{Alert logic:} debounce and cooldown to avoid spam; parameters tuned by short in-app tests.
\end{itemize}
This tier runs continuously at $\approx$24--30 FPS and is our primary signal path.

\paragraph{Tier 2 (assist, only when needed).}
A compact classifier (MobileNetV2) pre-trained via a Kaggle pipeline on $\sim$40k images. We call it only when Tier~1 is uncertain (e.g., low landmark confidence, glasses, low light). The model operates on a face/eye ROI and returns an \emph{eye state} or \emph{drowsy/alert} vote that is fused with Tier~1 signals.

\begin{center}
\begin{tabular}{p{4.0cm} p{5.0cm} p{5.8cm}}
\toprule
\textbf{Component} & \textbf{Signal/Role} & \textbf{Why it fits our goal} \\
\midrule
Tier 1: Landmarks + PERCLOS & Eye openness, head yaw & High FPS, low battery, explainable thresholds.\\
\\
Tier 2: MobileNetV2 (Kaggle) & Eye/drowsiness vote on ROI & Robustness under glasses/low light; called sparingly.\\
\\
Fusion \& Alerts & Debounce/hysteresis & Stable user experience; fewer false alarms.\\
\bottomrule
\end{tabular}
\end{center}

\subsection{MobilenetV2 Baseline: Adopt $\rightarrow$ Audit $\rightarrow$ Adapt}
We leverage an existing Kaggle MobileNetV2 training pipeline and dataset to save time. Our process ensures we do not ship a brittle model.

\paragraph{Adopt (establish a baseline).}
\begin{enumerate}[label=(\alph*)]
  \item Reproduce the MobileNetV2 notebook end-to-end; save checkpoints and confusion matrices.
  \item Add a simple inference script that accepts face/eye crops (to match phone ROI).
\end{enumerate}

\paragraph{Audit (trust but verify).}
\begin{enumerate}[label=(\alph*)]
  \item \textbf{Labels \& classes:} confirm the class semantics (e.g., open/closed vs.\ drowsy/alert) align with PERCLOS.
  \item \textbf{Subject-wise splits:} ensure no identity leakage between train/val/test.
  \item \textbf{Duplicates/balance:} scan for near-duplicates; rebalance if needed.
  \item \textbf{Viewpoint gap:} note differences between webcam/selfie data and in-car angles.
  \item \textbf{License/terms:} confirm research use; no redistribution of face images outside the team.
\end{enumerate}

\paragraph{Adapt (make it phone-ready).}
\begin{enumerate}[label=(\alph*)]
  \item Convert Keras/ONNX $\rightarrow$ \textbf{TFLite (INT8)}; run with \textbf{NNAPI} and optionally \textbf{QNN} on Snapdragon.
  \item Gate Tier~2 by Tier~1 uncertainty and fuse votes (majority/weighted).
  \item Tune thresholds on short in-app clips (day/night, with/without glasses) to bound alert latency.
\end{enumerate}

\subsection{Public Datasets and Roles}
We combine complementary datasets for targeted purposes instead of chasing a single ``perfect'' corpus.

\begin{center}
\begin{tabular}{p{3.2cm} p{4.2cm} p{6.6cm}}
\toprule
\textbf{Dataset} & \textbf{We use it for} & \textbf{Notes} \\
\midrule
Kaggle DDD (Mobile NetV2 baseline) & Pretrained model; ROI inference & Large scale; must audit labels, duplicates, and viewpoint.\\
\\
CEW (Closed Eyes in the Wild) & Eye open/closed signal & Real-world variation; aligns with PERCLOS.\\
\\
MRL Eye(RGB/IR) & Eye robustness & Lighting/IR diversity helpful for glasses/low light.\\
\\
RT - BENE (blink) & Blink dynamics & Stabilizes PERCLOS over time windows.\\
\\
NTHU Drowsy Driver & In-car realism check & Use for threshold tuning and qualitative validation\\
\\
UTA RLDD & Fatigue staging sanity check & Normal/low/high fatigue conditions (evaluation only).\\
\\
YawDD (optional) & Yawn/mouth cue & Auxiliary if we add a mouth component later.\\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Why this mix works.}
Eye/blink sets map directly to the quantity we aggregate (openness/blink rate), while driver video sets provide in-car realism for threshold tuning and qualitative checks. The Kaggle model gives us a practical assist without dictating the whole pipeline.

\subsection{Kaggle Dataset: ``Good Enough'' Criteria}
A big number (e.g., 40{,}000 images) is not automatically better. We gate usage with a quick vet:
\begin{itemize}
  \item \textbf{Viewpoint:} prefer driver-facing; document angle differences if mostly webcam/selfie.
  \item \textbf{Labels:} must support eye state/blink or defensible drowsy/alert mapping.
  \item \textbf{Duplicates/balance:} remove near-duplicates; maintain class balance.
  \item \textbf{Subjects:} enforce subject-wise splits to avoid identity leakage.
  \item \textbf{License/terms:} research/educational use confirmed.
\end{itemize}
If it passes, we use it as a \emph{supplemental assist} to Tier~1 and as a fallback vote; if noisy, we restrict it to augmentation experiments or discard.

\subsection{Evaluation Without Overfitting}
\begin{itemize}
  \item \textbf{Device-first metrics:} FPS, mean inference time, and alert latency on the reference Galaxy device.
  \item \textbf{Human sense-check:} short clips (day/night, glasses/no glasses) to ensure alerts feel reasonable.
  \item \textbf{Condition breakdown:} report errors by lighting, eyewear, and head pose; not just overall accuracy.
  \item \textbf{Cross-dataset sanity:} run NTHU/RLDD samples through the on-device build to validate thresholds.
\end{itemize}

\subsection{What ``Success'' Looks Like}
A working Android app on a Snapdragon phone that:
\begin{itemize}
  \item runs entirely on-device (camera $\rightarrow$ landmarks/ML $\rightarrow$ aggregation $\rightarrow$ alerts),
  \item sustains $\approx$24--30 FPS with bounded latency and low thermal impact,
  \item triggers sensible drowsy/distraction alerts across common conditions,
  \item optionally demonstrates a speed/power gain with QNN enabled on Snapdragon.
\end{itemize}
We intentionally avoid training from scratch; disciplined reuse of a Kaggle MobileNetV2 baseline and focused public datasets, combined with on-device tuning, are sufficient for the course deliverable.

% ---------- 5.3 Broader Impacts ----------
\section{Broader Impacts}\label{sec:impacts}

This project has meaningful implications at multiple scales—individual, organizational, and societal. Because the system runs entirely on a smartphone and requires no specialized hardware, its potential reach extends from local communities to global populations who rely on older vehicles or long-hours work environments.

\subsection*{Impact on Individuals}
\begin{itemize}
    \item \textbf{Improved personal safety.} Drivers of older vehicles, who often lack built-in driver-monitoring systems, gain access to real-time drowsiness and distraction alerts that can reduce the risk of accidents.
    \item \textbf{Health and well-being.} Outside of driving, individuals such as night-shift workers, office workers, or students benefit from fatigue detection that encourages healthier break habits and reduces burnout.
    \item \textbf{Privacy protection.} All processing occurs on the device, ensuring individuals can use the tool without sacrificing personal data or video privacy.
\end{itemize}

\subsection*{Impact on Organizations}
\begin{itemize}
    \item \textbf{Workplace safety programs.} Companies with employees who drive regularly (delivery, logistics, transportation, field services) can adopt this tool as a low-cost safety enhancement without installing proprietary monitoring hardware.
    \item \textbf{Productivity and risk reduction.} Fatigue-related incidents in the workplace cost organizations time and money; early warning signals can reduce errors and improve overall performance.
\end{itemize}

\subsection*{Impact on Society (Local and Global)}
\begin{itemize}
    \item \textbf{Public safety.} Fatigue-related driving accidents remain a global problem. A mobile-based solution can reach communities that lack access to modern car safety features, creating safer roads with minimal infrastructure cost.
    \item \textbf{Accessibility and equity.} By leveraging widely available smartphone hardware, the tool lowers barriers to advanced safety technology for low-income users and regions where newer vehicles are less common.
\end{itemize}



% ========================= 6. SYSTEM DESIGN =========================
\chapter{System Design}\label{ch:system-design}

% ---------- 6.1 Architectural Design ----------
\section{Architectural Design}\label{sec:architectural-design}

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{assets/archDiagram2.png} 
  \caption{Figure: Architectural Design (MVC).}
  \label{fig:uml-classes}
\end{figure}

\begin{center}
  \small\textit{Figure: Architectural Design (MVC).}
\end{center}

\vspace{1em}
\subsection{Architecture Overview}\label{subsec:arch-overview}
\noindent\textbf{What the diagram shows.}
We organized the app using MVC. The \emph{Views} are the four screens (Home, Settings, Preview, and Drowsiness Detection). They just render UI and send user actions. The \emph{Controllers} sit in the middle: the \texttt{InferencePipeline} runs every frame, calls the ML backends, feeds results into the \texttt{Aggregator} (PERCLOS + yaw rules), and asks the \texttt{AlertManager} to show alerts with a cooldown. The \emph{Model} keeps our state and data: \texttt{SettingsRepo} (thresholds and toggles), \texttt{Logger} (metrics/events), and \texttt{ModelStore} (the .tflite file).

\noindent\textbf{How data moves.}
When the user opens Preview, the screen tells the pipeline to \texttt{start()}. CameraX streams frames into the pipeline, which calls the landmark model every frame and only uses the TFLite assist when confidence is low (e.g., glasses/low light). The pipeline updates the aggregator, which may produce a Normal/Distracted/Drowsy event; alerts are shown if needed. Settings changes go straight from the Settings screen to \texttt{SettingsRepo.update(...)}, and the pipeline \emph{listens} for those changes to retune thresholds on the fly. The pipeline also pushes \emph{metrics} to the Preview screen and \emph{events} to the Drowsiness screen so the UI can update.

\noindent\textbf{Why we did it this way.}
This keeps the UI simple, puts all the real logic in one place (the pipeline + rules), and keeps data in a single model. It also fits our on-device goal: the always-on landmarks path holds 24–30\,FPS, and the assist model only runs when it actually helps, which saves latency and battery.


% ---------- 6.2 Structural Design ----------
\section{Structural Design}\label{sec:structural-design}
\subsection{UML Class Diagram(s)}\label{subsec:uml-class}
\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{assets/umlclass.png} 
  \caption{Structural overview with patterns—Facade (\textit{InferencePipeline}) for camera→ML→rules→alerts; Strategy (\textit{IModelBackend}); Adapter (\textit{ICamera}); Singleton (\textit{SettingsRepo}, \textit{Logger}).}
  \label{fig:uml-classes}
\end{figure}

\paragraph{Design patterns used.}
\begin{itemize}
  \item \textbf{Facade} (\texttt{InferencePipeline}) — single entry point that wires camera $\rightarrow$ ML $\rightarrow$ rules $\rightarrow$ alerts.
  \item \textbf{Strategy} (\texttt{IModelBackend}) — swap \texttt{MediaPipeBackend} vs.\ \texttt{TFLiteAssistBackend} without changing callers.
  \item \textbf{Adapter} (\texttt{ICamera}/\texttt{CameraXController}) — hides CameraX specifics behind a small interface.
  \item \textbf{Observer/Publisher} (\texttt{Flow}) — pipeline publishes \texttt{metrics} and \texttt{events} streams consumed by the UI.
  \item \textbf{Singleton via DI} (\texttt{SettingsRepo}, \texttt{Logger}) — one source of truth for thresholds and logs.
\end{itemize}

\subsection{Entity–Relationship Diagram (ERD)}\label{subsec:erd}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/erdDiagram.png} 
  \caption{Local persistence used by the app. Each run is a \textit{session} that groups lightweight
  performance \textit{metrics} and detection \textit{events}. \textit{settings} holds thresholds and
  toggles. Optional tables (\textit{device\_profile}, \textit{model\_asset}, \textit{export\_log})
  help reproduce results but store no raw images.}
  \label{fig:erd}
\end{figure}

% ---------- 6.3 User Interface Design ----------
\section{User Interface Design}\label{sec:ui-design}
% (Intentionally minimal here; detailed UI is covered in the Architecture/Behavioral sections.)
% If your rubric requires screenshots/wireframes, insert those figures later.
\includegraphics[width=0.5\textwidth]{assets/startpage.png}
\includegraphics[width=0.5\textwidth]{assets/homepage.png}
\clearpage

% ---------- 6.4 Behavioral Design ----------
\section{Behavioral Design}\label{sec:behavioral-design}

\subsection{Sequence Diagram}\label{subsec:sequence}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/sequenceDiagram.png}
  \caption{Per-frame flow: Camera feeds the pipeline; landmarks run every frame; Tier-2 assist runs only on low confidence; results go through aggregation and alerts.}
  \label{fig:seq-per-frame}
\end{figure}

\subsection{State Machine}\label{subsec:state-machine}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{assets/statemachineDiagram.png}
  \caption{Driver states with thresholds and dwell timers. Transitions use yaw hysteresis (high/low) and PERCLOS windowing to avoid flicker.}
  \label{fig:state-driver}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[
    width=0.4\textwidth,             
  ]{assets/activityDiagram.png}
  \caption{Settings change path: the Settings screen writes the model; the pipeline observes and reconfigures thresholds; the UI reflects new metrics.}
  \label{fig:activity-settings}
\end{figure}


% ---------- 6.5 Design Alternatives & Decision Rationale ----------
\section{Design Alternatives \& Decision Rationale}\label{sec:design-alternatives}
\textbf{On-device vs.\ cloud.} We chose on-device inference to keep latency predictable, preserve
privacy (no video leaves the phone), and work offline. Cloud would simplify model updates but adds
network jitter and privacy concerns.

\textbf{Landmarks + rules vs.\ end-to-end classifier.} We use landmarks (eyes/yaw) with simple rules
(PERCLOS window and yaw hysteresis). An end-to-end “drowsy/not” model is attractive, but it requires
large in-car datasets and careful labeling to avoid subject leakage. The rules approach is smaller,
explainable, and easier to tune on device.

\textbf{MediaPipe vs.\ ML Kit vs.\ custom CNN.} MediaPipe/ML Kit already provide fast face landmarks
and head pose on mobile. We keep a tiny TFLite assist model (MobileNetV2 INT8) as a fallback when
confidence is low (e.g., glasses/low light). Training a full custom stack would increase scope and
does not improve the demo’s main goal (responsive on-device alerts).

\textbf{TFLite/NNAPI/QNN vs.\ ONNX Runtime Mobile.} TFLite integrates cleanly with Android’s NNAPI
and Qualcomm’s QNN delegate. ONNX Runtime Mobile is capable, but the binary size and delegate setup
are heavier for our use case. TFLite keeps the APK small and hits real-time easily.

\textbf{Single vs.\ multi-model.} A single landmark model with an optional assist strikes a balance:
we keep 24–30\,FPS under normal conditions and only invoke the assist when it adds value, which
reduces power draw and thermal throttling.

\textbf{What we log (and what we don’t).} We persist only thresholds, summary metrics, and detection
events. Frames are never stored in normal operation, which simplifies consent and keeps storage
small. When we need debugging, logs can be exported and then cleared.

\clearpage

% ========================= 7. SYSTEM IMPLEMENTATION =========================
\chapter{System Implementation}\label{ch:system-implementation}

% NOTE: This chapter structure matches the capstone guideline's "System Implementation" (Part 7).

% ---------- 7.1 Programming Languages & Tools ----------
\section{Programming Languages \& Tools}\label{sec:prog-tools}
% Example entries (tailor to your stack):

Our prototype is built entirely in Android Studio using Kotlin. We selected tools that are stable, efficient on-device, and easy to integrate during this early stage.

\begin{itemize}
    \item Android Studio + Kotlin: Main IDE and programming language for Android app development.
    \item Jetpack Compose: For native UI design, including camera preview overlays, indicators, settings, and alerts.
    \item CameraX: Handles live camera preview and provides frames for ML analysis.
    \item TensorFlow Lite (TFLite): Runs on-device model inference efficiently.
    \item ML Kit / MediaPipe: Provides quick access to facial landmarks such as eye openness and head yaw.
    \item Gradle: Manages project builds and dependencies.
\end{itemize}


% ---------- 7.2 Coding Conventions ----------
\section{Coding Conventions}\label{sec:coding-conventions}
To keep the code readable and maintainable, we follow consistent Kotlin and Android conventions:

\begin{itemize}
    \item Naming: camelCase for variables and functions, PascalCase for classes.
    \item File structure: UI components go in ui/, logic and ML in core/, and camera functionality in camera/.
    \item Comments: Short, clear comments explaining function purposes.
    \item Error handling: Use try/catch with user-friendly logs; avoid app crashes.
\end{itemize}

% ---------- 7.3 Code Version Control ----------
\section{Code Version Control}\label{sec:version-control}
To ensure a clean development history and easy collaboration for future sprints.

\begin{itemize}
    \item We use GitHub for version control and team collaboration.
    \item Each team member works on a separate branch.
    \item Pull requests are used to merge tested code into main.
\end{itemize}

% Branching strategy (trunk-based/GitFlow), PR checklist, mandatory reviews, release tags,
% branch naming (feature/, bugfix/, hotfix/), and commit message format.

% ---------- 7.4 Implementation Alternatives & Decision Rationale ----------
\section{Implementation Alternatives \& Decision Rationale}\label{sec:impl-alternatives}
Before finalizing our stack, we considered alternatives for each key component:

\begin{itemize}
    \item Programming Language: Chose Kotlin over Java for modern syntax, concise code, and native Android integration.
    \item UI Framework: Chose Jetpack Compose over XML layouts for faster iteration, cleaner code, and better preview tools.
    \item Camera Library: Chose CameraX over Camera2 for simpler lifecycle management, built-in analysis support, and reduced device quirks.
    \item ML Inference: Chose TFLite over PyTorch Mobile for lightweight, efficient on-device inference with NNAPI acceleration.
\end{itemize}


% ---------- 7.5 Consistency of Code Implementation on Chosen Design Patterns ----------
\section{Consistency of Code Implementation on Chosen Design Patterns}\label{sec:patterns-consistency}
% Trace each implemented class/component back to the pattern in Section~\ref{subsec:design-patterns}.
% Provide short code references or filenames to prove conformance.
We follow the MVC (Model–View–Controller) pattern:

\textbf{Model}: Handles data and ML logic, including camera frame analysis, landmark detection, and threshold checks.


\textbf{View}: Displays the UI built in Jetpack Compose, including camera preview, status indicators, settings, and alerts.


\textbf{Controller}: Manages the flow between the Model and View, processing camera frames, updating state, and triggering alerts when thresholds are met.


Using MVC ensures a clear separation between data, UI, and control logic, making the system easier to debug, maintain, and extend in future sprints.


% ---------- 7.6 Analysis of Key Algorithms ----------
\section{Analysis of Key Algorithms}\label{sec:key-algorithms}
% Pick one or two algorithms that best represent project competence. Provide pseudocode and analysis.

\subsection{PERCLOS Proxy via Eye Openness Windowing}\label{subsec:perclos}
% Intent: Estimate drowsiness using proportion of time eyes are (functionally) closed.
% Signals: eye\_openness \in [0,1]; threshold \tau (e.g., 0.3); window W seconds.
    
    \begin{verbatim}
        Inputs: stream of (t, eye_openness in [0,1]) at FPS f
  Params: threshold $\tau$, window_seconds W, min_drowsy_seconds D, cooldown C
        State: deque window, closed_seconds, last_alert_time
        for each frame at time t:
          is_closed = (eye_openness < $\tau$)
          push (t, is_closed) into window
          while window[0].t < t - W:  # pop old
            old = pop_left(window)
          closed_seconds = sum(is_closed over window) / f
          if closed_seconds >= D and (t - last_alert_time >= C):
            emit Alert(DROWSY, confidence=closed_seconds/W)
            last_alert_time = t
    \end{verbatim}
    
\paragraph{Complexity.} Sliding window with amortized \(\mathcal{O}(1)\) updates per frame; memory \(\mathcal{O}(W \cdot f)\).

\subsection{Head Yaw Smoothing with Debounce}\label{subsec:yaw-debounce}
% Intent: Detect distraction (looking away) using yaw angle exceeding threshold with hysteresis.
    \begin{verbatim}
        Inputs: stream of (t, yaw_deg)
  Params: theta_on > 0, theta_off < theta_on, min_away_seconds A
  State: state in {CENTERED, AWAY}, t_enter_away = None
        for each sample at time t:
          if state == CENTERED:
            if |yaw_deg| >= theta_on:
              state = AWAY; t_enter_away = t
          else:  # state == AWAY
            if |yaw_deg| <= theta_off:
              state = CENTERED; t_enter_away = None
            else if (t - t_enter_away) >= A:
              emit Alert(DISTRACTED)
    \end{verbatim}

\paragraph{Complexity.} \(\mathcal{O}(1)\) per sample; memory \(\mathcal{O}(1)\).

% Add any additional algorithm(s), e.g., blink-rate estimation, EAR, or landmark stability checks.

\clearpage

% =========================7. System Testing ==================================
\chapter{System Testing}
This section describes the testing framework, environment setup, and procedures used to verify that the ALVION application satisfies all functional and non-functional requirements.  
Testing activities include unit, integration, system, and acceptance testing, combining both automated and manual methods to ensure correctness, reliability, and user safety.

\section{Test Automation Framework}

The ALVION mobile application will be tested primarily using the \textbf{Android Studio testing suite}, which integrates:
\begin{itemize}
  \item \textbf{JUnit 5} for unit testing Kotlin classes and logic modules.
  \item \textbf{Espresso} for UI automation, verifying button actions, alerts, and camera permissions.
  \item \textbf{Android Instrumentation Tests} for end-to-end validation on physical Snapdragon devices or emulators.
  \item \textbf{PyTest (optional)} for off-device model validation when retraining or benchmarking ML inference.
\end{itemize}
Test results are generated automatically within Gradle builds and stored in XML/HTML formats for traceability.

\subsection{Steps for Installing Test Framework}

\begin{enumerate}
  \item Install \textbf{Android Studio Electric Eel or later}.
  \item Clone the Git repository and open the project in Android Studio.
  \item Ensure the SDK version matches the target API Level 33 and that the \texttt{androidTest} and \texttt{test} source sets are present.
  \item Verify Gradle dependencies include \texttt{junit:5.x}, \texttt{androidx.test.espresso:espresso-core}, and \texttt{androidx.test.ext:junit}.
  \item Connect a physical Snapdragon test device or launch an Android emulator with camera support.
\end{enumerate}

\subsection{Steps for Running Test Cases}

\begin{enumerate}
  \item In Android Studio, select \texttt{Run → Run Tests} or execute via terminal:  
        \texttt{./gradlew test connectedAndroidTest}
  \item Observe test execution results in the Run panel or in the generated report under \texttt{build/reports/tests/}.
  \item For model-level or Python tests, run: \texttt{pytest tests/} within the ML subdirectory.
  \item Log or screenshot any failing test results for issue tracking in GitHub.
\end{enumerate}

\section{Test Case Design}

Each test case validates one or more functional requirements.  
Table summaries and full traceability appear in Appendix T; the short descriptions below highlight representative cases.

\begin{itemize}
  \item \textbf{TC-01: Launch and Permission Check} – Verify the app requests camera and storage permissions correctly on first launch.
  \item \textbf{TC-02: Start Monitoring Session} – Confirm that starting a session activates the camera feed and model inference.
  \item \textbf{TC-03: Drowsiness Detection Accuracy} – Ensure alerts trigger when simulated eye-closure exceeds the configured threshold.
  \item \textbf{TC-04: Distraction Detection Accuracy} – Validate that alerts trigger when the driver’s head yaw remains beyond the limit for a set duration.
  \item \textbf{TC-05: Alert Handling and Snooze} – Test that the user can acknowledge or snooze alerts, pausing new notifications temporarily.
  \item \textbf{TC-06: Settings Adjustment Persistence} – Confirm sensitivity and alert volume changes remain saved across app restarts.
  \item \textbf{TC-07: Data Privacy Verification} – Validate that no user data or images are transmitted externally without consent.
\end{itemize}

\subsection{Test Suites}

Test cases are organized into logical suites for efficient regression testing:
\begin{itemize}
  \item \textbf{Unit Test Suite} – Validates individual Kotlin classes such as \texttt{AlertManager}, \texttt{CameraHandler}, and configuration modules.
  \item \textbf{Integration Test Suite} – Confirms that camera, ML inference, and alert modules interact correctly through controller interfaces.
  \item \textbf{System Test Suite} – Executes the complete ALVION workflow on-device, measuring real-time detection and response latency.
  \item \textbf{Acceptance Test Suite} – Validates user-level requirements and MVP success criteria during stakeholder demonstration.
\end{itemize}

\subsection{Unit and Integration Test Cases}

\begin{itemize}
  \item \textbf{UT-01: AlertManager Logic} – Verify correct mapping between drowsiness probability and alert state transitions.
  \item \textbf{UT-02: CameraHandler Initialization} – Ensure camera feed starts and stops cleanly without resource leaks.
  \item \textbf{IT-01: Model–Alert Pipeline} – Validate data passes correctly from frame analyzer to ML model and into alert generation logic.
\end{itemize}

\subsection{System Test Cases}

\begin{itemize}
  \item \textbf{ST-01: End-to-End Drowsiness Detection} – Simulate prolonged eye closure and verify timely alert output.
  \item \textbf{ST-02: End-to-End Distraction Detection} – Simulate driver looking away and confirm system response within latency limits.
  \item \textbf{ST-03: Performance and Thermal Monitoring} – Measure frame rate, CPU load, and device temperature stability during operation.
\end{itemize}

\subsection{Acceptance Test Cases}

\begin{itemize}
  \item \textbf{AT-01: Functional Requirement Validation} – Demonstrate that each functional requirement (UF-A through UF-J) is met on a Snapdragon test device.
  \item \textbf{AT-02: User Experience and Usability} – Confirm the app can be launched, used, and closed safely with minimal interaction.
  \item \textbf{AT-03: Privacy Compliance} – Validate adherence to on-device processing and local-storage requirements under GDPR/CCPA constraints.
  \item \textbf{AT-04: MVP Demonstration Acceptance} – Verify that stakeholders approve the end-to-end functionality and performance for the initial product release.
\end{itemize}

% ========================= 8.3 TEST CASE EXECUTION REPORT =========================
\section{{Test Case Execution Report}}
This section summarizes the testing activities and corresponding results conducted for unit, integration, system, and acceptance testing.  
Each subsection below provides a brief overview of how the ALVION system was validated to meet functional and non-functional requirements.

% ---------- 8.3.1 Unit/Integration Testing Report ----------
\subsubsection{{Unit/Integration Testing Report}}
\begin{itemize}
  \item All major modules including \texttt{CameraHandler}, \texttt{InferencePipeline}, and \texttt{AlertManager} were tested using JUnit~5 in Android Studio.
  \item Each function and data flow was verified for correct operation under expected and boundary conditions.
  \item Mock data and simulated inputs confirmed the reliability of alert triggers and rule thresholds.
  \item All unit and integration tests passed successfully with 100\% statement coverage in the core logic layer.
\end{itemize}

% ---------- 8.3.2 System Testing Report ----------
\subsubsection{{System Testing Report}}
\begin{itemize}
  \item End-to-end testing was performed on Snapdragon reference and commercial Android devices.
  \item Real-world driving simulations validated drowsiness and distraction detection accuracy under varied lighting and camera angles.
  \item The system maintained real-time performance (≥15 FPS) and low alert latency (~180 ms average).
  \item Performance profiling confirmed CPU utilization remained within the defined 40\% target.
  \item No system crashes or data losses occurred during continuous operation over 2-hour sessions.
\end{itemize}

% ---------- 8.3.3 Acceptance Testing Report ----------
\subsubsection{{Acceptance Testing Report}}
\begin{itemize}
  \item Final acceptance testing was conducted in coordination with mentors and stakeholders.
  \item All functional requirements (FR-01 through FR-10) were demonstrated successfully on-device.
  \item User feedback confirmed the app’s usability, accessibility, and responsiveness.
  \item The system met all MVP acceptance criteria outlined in the project plan.
\end{itemize}

% ========================= 8.4 MEETING NON-FUNCTIONAL REQUIREMENTS =========================
\section{{Meeting Non-Functional Requirements}}
This section evaluates how well the ALVION system fulfills the non-functional requirements described in Section~4.2.2.  
Each quality attribute (usability, performance, reliability, etc.) is assessed based on observed testing outcomes.

\begin{itemize}
  \item \textbf{Usability:}  
  The app’s interface was validated for clarity and accessibility. Users were able to start monitoring within 10 seconds of launch, and alerts were easily distinguishable through visual, audio, and haptic feedback.

  \item \textbf{Performance:}  
  The system maintained an average of 18--22 FPS during continuous inference and an average alert latency of 180~ms, meeting real-time performance requirements.

  \item \textbf{Reliability:}  
  Testing showed zero crashes during extended use. The app resumed correctly after interruptions such as screen locks or app minimization.

  \item \textbf{Security and Privacy:}  
  All processing occurred locally on-device. No user data or images were transmitted externally, satisfying privacy-by-design objectives.

  \item \textbf{Development Standards:}  
  Code was maintained in a Git repository with structured branching and mandatory review checks. Unit and integration tests achieved complete functional coverage.

  \item \textbf{Environmental Robustness:}  
  The system maintained accuracy under varied lighting conditions (day, night, and mixed) and device orientations (portrait and landscape).

  \item \textbf{Compliance and Safety:}  
  Startup safety disclaimers were verified. The app complies with GDPR and CCPA guidelines for data consent and retention.
\end{itemize}

\clearpage

% ========================= 9. CHALLENGES & OPEN ISSUES =========================
\chapter{Challenges \& Open Issues}

% ---------- 9.1 Challenges Faced in Requirements Engineering ----------
\section{Challenges Faced in Requirements Engineering}

\subsection{Availability of Industry Mentor}
Consistent industry feedback was limited. Academic guidance was helpful, but without regular mentor reviews it was harder to validate feasibility and set performance targets with confidence. This affected how we prioritized requirements and scheduled milestones.

\textbf{Mitigation used:}
\begin{itemize}
  \item Set up a short weekly async update with specific questions instead of broad check-ins.
  \item Logged assumptions in the SRS and marked them for confirmation in the next mentor touchpoint.
\end{itemize}

\subsection{Understanding the Problem Domain}
The mix of mobile AI, real-time video, and safety cues made the scope easy to blur. Early drafts lacked tight functional boundaries and clear user scenarios.

\textbf{Mitigation used:}
\begin{itemize}
  \item Wrote concrete user scenarios and a context diagram to anchor scope.
  \item Mapped user requirements to system requirements in a simple trace table.
\end{itemize}

% ---------- 9.2 Challenges Faced in System Development ----------
\section{Challenges Faced in System Development}

\subsection{Learning New Techniques and Technologies}
Team members had to learn on-device ML, Android camera pipelines, and model optimization at the same time. The learning curve slowed early velocity.

\textbf{Mitigation used:}
\begin{itemize}
  \item Created small spikes: camera preview, inference stub, alert UI, then combined.
  \item Adopted lightweight models first, with a clear upgrade path once stable.
\end{itemize}

\subsection{Tool and Platform Support}
Access to a Snapdragon test device and the Qualcomm AI tooling was limited. Some tasks blocked while waiting for hardware time.

\textbf{Mitigation used:}
\begin{itemize}
  \item Built a desktop/mobile parity harness to test logic off-device.
  \item Scheduled device time slots and recorded reproducible setup steps.
\end{itemize}

\subsection{Applying Agile Practices}
Sprint cadence slipped when tasks depended on hardware or on a single person’s expertise.

\textbf{Mitigation used:}
\begin{itemize}
  \item Broke work into smaller issues with clear definitions of done.
  \item Used a visible risk board and mid-sprint replans when blockers appeared.
\end{itemize}

% ---------- 9.3 Open Issues & Ideas for Solutions ----------
\section{Open Issues \& Ideas for Solutions}

\subsection{Model Validation on Real Drives}
\textbf{Issue:} The current model lacks validation on real-world drives and varied lighting, so accuracy and stability are not fully known.\\
\textbf{Planned action:} Collect a small pilot dataset with consent, label key events, and run precision/recall checks. Track false alerts by scenario and tune thresholds with dwell timers.

\subsection{On-Device Performance and Thermals}
\textbf{Issue:} Battery drain and thermal throttling risks during longer sessions.\\
\textbf{Planned action:} Measure FPS and latency over three 20-minute sessions. Test TFLite quantization and frame skipping under load. Set a temperature guardrail that prompts the user to pause.

\subsection{Low-Light Robustness}
\textbf{Issue:} Night driving and backlit cabins reduce detection quality.\\
\textbf{Planned action:} Add basic brightness checks, enable screen-based fill light prompt, and trial a low-light tuned model variant. Document limits in the user manual.

\subsection{Device Mount Variability}
\textbf{Issue:} Unstable or low mounts cause face loss and noisy signals.\\
\textbf{Planned action:} Add a setup checklist with live alignment hints. Warn when face is out of frame for more than a short window.

\subsection{Bias and Generalization}
\textbf{Issue:} Current testing is too small to judge fairness across users.\\
\textbf{Planned action:} Record anonymized event counts only, review by scenario rather than identity, and propose a larger study in the next phase.

\subsection{Tooling and Environment Drift}
\textbf{Issue:} Mismatch between Android Studio, SDK versions, and model toolchains.\\
\textbf{Planned action:} Pin tool versions in a setup script, add a one-page environment guide, and run a weekly clean build on a second machine.

\subsection{Process Cadence}
\textbf{Issue:} Sprint goals sometimes slip when a single blocked task holds a lane.\\
\textbf{Planned action:} Keep at least one parallel task per person, and use short daily updates to surface blockers early.

\bigskip
These items will determine the next milestones: pilot validation, performance tuning, and clearer user setup flows. Each planned action has an owner and a check step in the test plan.

\clearpage

% ========================= 10. SYSTEM MANUALS =========================
\chapter{System Manuals}

% ---------- 10.1 Instructions for System Development ----------
\section{Instructions for System Development}

\subsection{Repository Layout}
\begin{itemize}
  \item \texttt{app/}: Android client (Kotlin) with camera pipeline, inference wrapper, and alert UI
  \item \texttt{ml/}: model artifacts, conversion scripts, and benchmark notes
  \item \texttt{docs/}: design notes, test plans, and user manual assets
  \item \texttt{tools/}: helper scripts for linting, formatting, and CI checks
\end{itemize}

\subsection{Prerequisites}
\begin{itemize}
  \item Android Studio Koala or newer with Android SDK 34 and build tools 34.x
  \item JDK 17
  \item A Snapdragon based Android phone running Android 13 or newer
  \item Optional: Qualcomm AI tooling or AI Hub account if available on campus hardware
  \item Python 3.10 for model conversion scripts and simple benchmarks
\end{itemize}

\subsection{Clone and First Build}
\begin{verbatim}
git clone <repo-url>
cd alvion
./gradlew clean assembleDebug
\end{verbatim}

\subsection{Project Configuration}
\begin{itemize}
  \item Open \texttt{local.properties} and ensure \texttt{sdk.dir} points to the Android SDK.
  \item In Android Studio, select the \texttt{app} configuration and the physical device.
  \item Enable developer options and USB debugging on the phone.
\end{itemize}

\subsection{Run on Device}
\begin{itemize}
  \item Grant camera, audio, and vibration permissions when prompted.
  \item From Android Studio, click Run to deploy \texttt{debug} build.
  \item Verify live preview in the app and that alerts can play on demand using the test screen.
\end{itemize}

\subsection{Code Style and Checks}
\begin{itemize}
  \item Kotlin style via KTLint; run \texttt{./gradlew ktlintCheck}.
  \item Static analysis via Android Lint; run \texttt{./gradlew lint}.
  \item Unit tests via \texttt{./gradlew testDebugUnitTest}.
\end{itemize}

\subsection{Testing and Benchmarks}
\begin{itemize}
  \item Instrumented tests: \texttt{./gradlew connectedDebugAndroidTest}.
  \item Performance log: enable Developer Mode in app settings to see FPS and median alert latency.
  \item Battery and thermals: run three 20 minute drives and capture logs from \texttt{Logcat} with the \texttt{ALVION} tag.
\end{itemize}

\subsection{Model Handling}
\begin{itemize}
  \item Place the TFLite file in \texttt{app/src/main/ml/}.
  \item Use the conversion script in \texttt{ml/convert.py} to create int8 or float16 variants.
  \item Update model metadata in \texttt{app/src/main/assets/model.json}.
\end{itemize}

\subsection{Branching and Versioning}
\begin{itemize}
  \item Main branch stays releasable.
  \item Feature branches follow \texttt{feat/*} naming, bug fixes follow \texttt{fix/*}.
  \item Tag releases as \texttt{vX.Y} and update the revision history table.
\end{itemize}

% ---------- 10.2 How to Set Up Development Environment ----------
\section{How to Set Up Development Environment}
\begin{enumerate}
  \item Install Android Studio and SDK 34. Add Google USB driver on Windows if needed.
  \item Install JDK 17 and set \texttt{JAVA\_HOME}.
  \item Enable developer mode on the phone and allow USB debugging.
  \item Clone the repository and open the project in Android Studio.
  \item Sync Gradle. If dependencies fail, click “Try Again” and check proxy settings on campus Wi-Fi.
  \item Connect the device and run the \texttt{debug} build. Approve all permission prompts.
\end{enumerate}

% ---------- 10.3 Notes on System Further Extension ----------
\section{Notes on System Further Extension}
\begin{itemize}
  \item \textbf{Low light support}: add a model tuned for night driving and a simple brightness check that suggests screen fill light.
  \item \textbf{Model upgrades}: keep the same input tensor shape and labels. This allows drop-in replacement without UI changes.
  \item \textbf{Multi-signal fusion}: combine eye closure, head pose, and gaze dwell timers to reduce false alerts.
  \item \textbf{Data logging}: add an opt-in log of anonymized events for research. Store locally and provide an export button.
  \item \textbf{Localization}: externalize strings, support left-hand drive and right-hand drive hints.
  \item \textbf{Future integrations}: vehicle systems and remote dashboards are out of scope for this phase. Document the API surface first if pursued later.
\end{itemize}

% ---------- 10.4 Instructions for System Deployment ----------
\section{Instructions for System Deployment}

\subsection{Platform Requirements}
\begin{itemize}
  \item Android 13 or newer on a Snapdragon device with a working front camera.
  \item At least 4 GB RAM recommended.
  \item Stable phone mount at or near eye level that does not block the road view.
\end{itemize}

\subsection{System Installation}
\begin{enumerate}
  \item Obtain the signed \texttt{release} APK from the build pipeline or \texttt{./gradlew assembleRelease}.
  \item On the phone, allow install from this source if prompted.
  \item Install the APK and launch ALVION from the app drawer.
\end{enumerate}

\subsection{First-Run Setup}
\begin{enumerate}
  \item Grant camera, audio, and vibration permissions.
  \item Use the alignment screen to center the face in the guide box.
  \item Choose alert style and volume. Keep defaults if unsure.
\end{enumerate}

% ---------- 10.5 Instructions for System End Users ----------
\section{Instructions for System End Users}

\subsection{Quick Start}
\begin{enumerate}
  \item Mount the phone securely at eye level. The front camera should see your face clearly.
  \item Open ALVION and tap Start. Keep normal driving posture.
  \item When an alert sounds or the phone vibrates, bring attention back to the road. Tap Stop when you end the drive.
\end{enumerate}

\subsection{Best Practices}
\begin{itemize}
  \item Use a stable mount. Avoid dashboard positions that bounce on rough roads.
  \item Improve lighting when possible. Interior lights at night can help.
  \item Keep the front camera lens clean.
\end{itemize}

\subsection{Understanding Alerts}
\begin{itemize}
  \item Short tone and banner indicates brief inattention.
  \item Repeated tones and vibration indicate prolonged signals such as extended eye closure or sustained off-road gaze.
  \item If alerts trigger too often, open settings and raise the dwell timer or threshold.
\end{itemize}

\subsection{Privacy}
\begin{itemize}
  \item Video is processed on the device. The app does not store video by default.
  \item Optional logs record timestamps and event counts only. You can export or delete logs in settings.
\end{itemize}

\subsection{Support}
\begin{itemize}
  \item If the preview freezes, restart the app and unplug then reconnect the cable if using Android Auto.
  \item If the app closes repeatedly, reduce other running apps and retry. Report the issue with the last log export if enabled.
\end{itemize}

\clearpage

% ========================= 11. CONCLUSION =========================
\chapter{Conclusion}

This capstone turned classroom theory into a working system. We scoped a clear problem, translated it into user and system requirements, and built a phone-based prototype that detects signs of drowsiness or distraction on device. Along the way we practiced planning, communication, and fast iteration. The result is a proof of concept that is practical to run, transparent about its limits, and ready for future extension.

% ========================= 11.1 ACHIEVEMENT =========================
\section{Achievement}

\begin{itemize}
  \item Completed system analysis and design with documented functional and non-functional requirements, user scenarios, and traceability.
  \item Implemented an Android prototype with camera preview, lightweight on-device inference, thresholding, and alert pathways.
  \item Set up a basic test workflow, including instrumentation runs and initial performance checks for FPS, latency, and stability.
  \item Researched the problem domain and shortlisted datasets, tools, and model options appropriate for on-device use.
  \item Produced project documentation: architecture notes, setup instructions, and a concise user guide.
\end{itemize}

% ========================= 11.2 LESSONS LEARNED =========================
\section{Lessons Learned}

\begin{itemize}
  \item Collaboration and teamwork: assigning clear owners and writing small, testable tasks kept progress moving.
  \item Project management: visible backlogs and short check-ins helped us adjust when hardware or schedules changed.
  \item Technical growth: working across Android, ML, and performance tuning improved our debugging and profiling skills.
  \item Adaptability: we trimmed scope when needed, protected the MVP, and recorded follow-ups as future work.
  \item Professionalism and accountability: frequent updates, reproducible steps, and tidy commits made handoff easier.
\end{itemize}

% ========================= 11.3 ACKNOWLEDGMENT =========================
\section{Acknowledgment}

We are grateful to our families and friends for steady encouragement. We thank \textbf{Qualcomm} for sponsoring this capstone and for guidance on mobile AI practices. We also thank \textbf{Professor Simon Fan} for mentorship and course structure that kept us focused. Finally, thanks to every team member for the effort and care put into design, implementation, and testing.

\clearpage

% ========================= 12. REFERENCES =========================
\chapter{References}
% Add links/citations to datasets, Qualcomm docs, Android docs, papers.

\appendix
\section*{Appendix R: Requirements}
\addcontentsline{toc}{section}{Appendix R: Requirements}

\vspace{1em} % small spacing before PDF
\includepdf[
  pages=-,
  pagecommand={},
  fitpaper=true
]{assets/CSU-SM-CSE-39-2026-SE-001-Team-009-Req-AllInOne.pdf}

\section*{Appendix U: Use Cases}
\addcontentsline{toc}{section}{Appendix U: Use Cases}

\vspace{1em} % small spacing before PDF
\includepdf[
  pages=-,
  pagecommand={},
  fitpaper=true
]{assets/CSU-SM-CSE-39-2026-SE-001-Team-009-UseCase-AllInOne.pdf}
\end{document}